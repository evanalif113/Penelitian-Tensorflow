{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "85c35c01",
        "execution_start": 1764810434480,
        "execution_millis": 4831,
        "execution_context_id": "e66bfa74-fbe1-4a4f-88f3-32a38d74febb",
        "cell_id": "d37be145408347e6a095923542f955ac",
        "deepnote_cell_type": "code"
      },
      "source": "import openmeteo_requests\nimport requests_cache\nimport pandas as pd\nfrom retry_requests import retry\nimport matplotlib.pyplot as plt\nimport os\nfrom datetime import datetime, timedelta\n\ndef setup_client():\n    \"\"\"\n    Menyiapkan client API dengan fitur Cache dan Retry.\n    Tujuannya agar koneksi stabil dan hemat kuota.\n    \"\"\"\n    # Buat cache bernama '.cache', data disimpan selama 3600 detik (1 jam)\n    cache_session = requests_cache.CachedSession('.cache', expire_after=3600)\n    \n    # Jika gagal connect, coba lagi (retry) sampai 5x\n    retry_session = retry(cache_session, retries=5, backoff_factor=0.2)\n    \n    # Return objek client yang siap pakai\n    return openmeteo_requests.Client(session=retry_session)",
      "block_group": "d37be145408347e6a095923542f955ac",
      "execution_count": 1,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "268eebe8",
        "execution_start": 1764810570610,
        "execution_millis": 1,
        "execution_context_id": "e66bfa74-fbe1-4a4f-88f3-32a38d74febb",
        "cell_id": "0ea444afb4ce4e5490b656b4afc61765",
        "deepnote_cell_type": "code"
      },
      "source": "# ==========================================\n# 2. FETCH DATA PER CHUNK\n# ==========================================\ndef fetch_chunk(client, lat, lon, start_date, end_date):\n    url = \"https://archive-api.open-meteo.com/v1/archive\"\n    params = {\n        \"latitude\": lat,\n        \"longitude\": lon,\n        \"start_date\": start_date,\n        \"end_date\": end_date,\n        \"hourly\": [\n            \"temperature_2m\", \n            \"relative_humidity_2m\",\n            \"dew_point_2m\",\n            \"rain\",\n            \"wind_speed_10m\",\n            \"wind_direction_10m\",\n            \"surface_pressure\",\n            \"weather_code\"\n            ],\n        \"timezone\": \"Asia/Jakarta\"\n    }\n    \n    print(f\"   ‚è≥ Mengambil: {start_date} s.d {end_date}...\")\n    try:\n        responses = client.weather_api(url, params=params)\n        return process_data(responses[0]) # Langsung olah jadi DataFrame\n    except Exception as e:\n        print(f\"   ‚ùå Gagal pada chunk ini: {e}\")\n        return None\n\n# ==========================================\n# 3. PROCESS DATA (SAMA SEPERTI SEBELUMNYA)\n# ==========================================\ndef process_data(response):\n    hourly = response.Hourly()\n    \n    date_range = pd.date_range(\n        start = pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n        end = pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n        freq = pd.Timedelta(seconds=hourly.Interval()),\n        inclusive = \"left\"\n    )\n\n    df = pd.DataFrame(data = {\n        \"date\": date_range,\n        \"temperature\": hourly.Variables(0).ValuesAsNumpy(),\n        \"humidity\": hourly.Variables(1).ValuesAsNumpy(),\n        \"dewpoint\":hourly.Variables(2).ValuesAsNumpy(),\n        \"rain_mm\": hourly.Variables(3).ValuesAsNumpy(),\n        \"wind_speed\": hourly.Variables(4).ValuesAsNumpy(),\n        \"wind_direction\": hourly.Variables(5).ValuesAsNumpy(),\n        \"pressure\": hourly.Variables(6).ValuesAsNumpy(),\n        \"weather_code\": hourly.Variables(7).ValuesAsNumpy()\n    })\n\n    df = df.set_index('date')\n    df.index = df.index.tz_convert('Asia/Jakarta')\n    return df\n\n# ==========================================\n# 4. LOGIKA UTAMA: CHUNKING & MERGING\n# ==========================================\ndef fetch_long_period_data(client, lat, lon, start_str, end_str, folder_tujuan, chunk_years=10):\n    \n    if not os.path.exists(folder_tujuan):\n        os.makedirs(folder_tujuan)\n\n    start_date = datetime.strptime(start_str, \"%Y-%m-%d\")\n    final_end_date = datetime.strptime(end_str, \"%Y-%m-%d\")\n    \n    all_files = []\n    \n    current_start = start_date\n    \n    print(f\"üöÄ Memulai Misi Pengambilan Data 75 Tahun ({chunk_years} tahunan)...\")\n\n    while current_start < final_end_date:\n        # Hitung tanggal akhir chunk ini\n        # Misal start 1950, tambah 10 tahun -> 1960. Dikurang 1 hari biar gak overlap.\n        current_end = current_start.replace(year=current_start.year + chunk_years) - timedelta(days=1)\n        \n        # Jika current_end melebihi batas akhir request, pakai batas akhir request\n        if current_end > final_end_date:\n            current_end = final_end_date\n            \n        # Format string untuk API\n        s_str = current_start.strftime(\"%Y-%m-%d\")\n        e_str = current_end.strftime(\"%Y-%m-%d\")\n        \n        # Nama file sementara\n        chunk_filename = os.path.join(folder_tujuan, f\"temp_{s_str}_{e_str}.csv\")\n        \n        # Cek apakah file sudah ada? (Resume Capability)\n        if os.path.exists(chunk_filename):\n            print(f\"‚è© Skip: {s_str} - {e_str} (Sudah ada)\")\n            all_files.append(chunk_filename)\n        else:\n            # Ambil Data\n            df_chunk = fetch_chunk(client, lat, lon, s_str, e_str)\n            if df_chunk is not None:\n                df_chunk.to_csv(chunk_filename)\n                print(f\"   ‚úÖ Tersimpan: {chunk_filename}\")\n                all_files.append(chunk_filename)\n            else:\n                print(\"   ‚ö†Ô∏è Chunk ini dilewati karena error.\")\n        \n        # Lanjut ke periode berikutnya\n        current_start = current_end + timedelta(days=1)\n\n    print(\"\\nüîó Menggabungkan semua pecahan data...\")\n    \n    # Gabungkan semua CSV jadi satu\n    df_list = []\n    for f in all_files:\n        df = pd.read_csv(f, index_col='date', parse_dates=True)\n        df_list.append(df)\n        \n    if df_list:\n        df_final = pd.concat(df_list)\n        df_final = df_final.sort_index() # Urutkan waktu biar rapi\n        # Hapus duplikat jika ada irisan\n        df_final = df_final[~df_final.index.duplicated(keep='first')]\n        \n        print(f\"üéâ SUKSES BESAR! Total Data: {len(df_final)} baris.\")\n        print(f\"   Mulai: {df_final.index.min()}\")\n        print(f\"   Akhir: {df_final.index.max()}\")\n        return df_final\n    else:\n        return None",
      "block_group": "8a8d70e91c8c471792446bf51a532bcf",
      "execution_count": 4,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "363900fa",
        "execution_start": 1764810576797,
        "execution_millis": 17591,
        "execution_context_id": "e66bfa74-fbe1-4a4f-88f3-32a38d74febb",
        "cell_id": "27a58b5daec24dc1b9fbb07b5f1eb170",
        "deepnote_cell_type": "code"
      },
      "source": "# ==========================================\n# EKSEKUSI\n# ==========================================\nif __name__ == \"__main__\":\n    # Konfigurasi\n    LAT = -7.736436737566032, \n    LON = 109.6460550796716\n    MULAI = \"1950-01-01\"\n    AKHIR = \"2025-12-03\"\n    \n    FOLDER = \"open_meteo_climate\"\n    FILE_FINAL = \"kebumen_75tahun_lengkap.csv\"\n\n    client = setup_client()\n    \n    # Jalankan Fetching Bertahap (per 10 tahun)\n    df_lengkap = fetch_long_period_data(client, LAT, LON, MULAI, AKHIR, FOLDER, chunk_years=10)\n    \n    if df_lengkap is not None:\n        # Simpan Hasil Akhir\n        path_final = os.path.join(FOLDER, FILE_FINAL)\n        df_lengkap.to_csv(path_final)\n        print(f\"üíæ File Gabungan Tersimpan: {path_final}\")\n        \n        # Hapus file temp (Opsional, kalau mau hemat disk)\n        # import glob\n        # for f in glob.glob(f\"{FOLDER}/temp_*.csv\"):\n        #    os.remove(f)",
      "block_group": "5904683d31a74f6c8c2cf711a0f1d95b",
      "execution_count": 7,
      "outputs": [
        {
          "name": "stdout",
          "text": "üöÄ Memulai Misi Pengambilan Data 75 Tahun (10 tahunan)...\n‚è© Skip: 1950-01-01 - 1959-12-31 (Sudah ada)\n‚è© Skip: 1960-01-01 - 1969-12-31 (Sudah ada)\n‚è© Skip: 1970-01-01 - 1979-12-31 (Sudah ada)\n‚è© Skip: 1980-01-01 - 1989-12-31 (Sudah ada)\n‚è© Skip: 1990-01-01 - 1999-12-31 (Sudah ada)\n‚è© Skip: 2000-01-01 - 2009-12-31 (Sudah ada)\n‚è© Skip: 2010-01-01 - 2019-12-31 (Sudah ada)\n   ‚è≥ Mengambil: 2020-01-01 s.d 2025-12-03...\n   ‚úÖ Tersimpan: open_meteo_climate/temp_2020-01-01_2025-12-03.csv\n\nüîó Menggabungkan semua pecahan data...\nüéâ SUKSES BESAR! Total Data: 665544 baris.\n   Mulai: 1950-01-01 01:00:00+08:00\n   Akhir: 2025-12-03 23:00:00+07:00\nüíæ File Gabungan Tersimpan: open_meteo_climate/kebumen_75tahun_lengkap.csv\n",
          "output_type": "stream"
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "1672da214fd441a2801448e3e2e86dae",
        "deepnote_cell_type": "code"
      },
      "source": "",
      "block_group": "5448ec3bdb4841aca2d1ab0468ec1e2c",
      "execution_count": null,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=672c3bf7-b636-46e8-b7c0-4f6feec29313' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote_notebook_id": "540be87afd9e4d5ca3ed273655f9f967"
  }
}